name: Monitoring and Alerting Pipeline

on:
  schedule:
    - cron: '*/15 * * * *'  # Every 15 minutes
    - cron: '0 */6 * * *'   # Every 6 hours for detailed reports
  workflow_dispatch:
    inputs:
      monitoring_scope:
        description: 'Monitoring scope'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - infrastructure
        - applications
        - security
        - performance
      environment:
        description: 'Environment to monitor'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - development
        - staging
        - production
      alert_level:
        description: 'Alert level threshold'
        required: true
        default: 'warning'
        type: choice
        options:
        - info
        - warning
        - error
        - critical

env:
  AZURE_RESOURCE_GROUP_PREFIX: ai-multiagent-lab-rg
  LOG_ANALYTICS_WORKSPACE: ai-multiagent-lab-logs

jobs:
  # Job 1: Infrastructure Monitoring Matrix
  infrastructure-monitoring:
    name: Infrastructure Monitoring
    runs-on: ubuntu-latest
    if: github.event.inputs.monitoring_scope == 'all' || github.event.inputs.monitoring_scope == 'infrastructure' || github.event.inputs.monitoring_scope == ''
    strategy:
      matrix:
        environment: [development, staging, production]
        resource_type: [aks, cosmosdb, functions, networking, storage]
        metric_category: [availability, performance, cost, security]
      fail-fast: false

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Login to Azure
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Setup Azure CLI
      uses: azure/CLI@v1
      with:
        azcliversion: 2.76.0

    - name: Install monitoring tools
      run: |
        pip install azure-monitor-query azure-mgmt-monitor azure-identity
        pip install pandas matplotlib seaborn

    - name: Query infrastructure metrics
      run: |
        python << EOF
        import json
        import os
        from datetime import datetime, timedelta
        
        # Simulate infrastructure monitoring
        environment = "${{ matrix.environment }}"
        resource_type = "${{ matrix.resource_type }}"
        metric_category = "${{ matrix.metric_category }}"
        
        print(f"Monitoring {resource_type} {metric_category} in {environment}")
        
        # Mock metrics data
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "environment": environment,
            "resource_type": resource_type,
            "metric_category": metric_category,
            "metrics": {
                "availability": 99.9 if metric_category == "availability" else None,
                "response_time": 150 if metric_category == "performance" else None,
                "cost": 125.50 if metric_category == "cost" else None,
                "security_score": 85 if metric_category == "security" else None
            },
            "status": "healthy",
            "alerts": []
        }
        
        # Check thresholds and generate alerts
        if metric_category == "availability" and metrics["metrics"]["availability"] < 99.5:
            metrics["alerts"].append({
                "level": "warning",
                "message": f"Availability below threshold: {metrics['metrics']['availability']}%"
            })
        
        if metric_category == "performance" and metrics["metrics"]["response_time"] > 200:
            metrics["alerts"].append({
                "level": "warning",
                "message": f"Response time above threshold: {metrics['metrics']['response_time']}ms"
            })
        
        # Save metrics
        with open(f"metrics-{environment}-{resource_type}-{metric_category}.json", "w") as f:
            json.dump(metrics, f, indent=2)
        
        print(f"Metrics saved for {resource_type} {metric_category}")
        if metrics["alerts"]:
            print(f"Alerts generated: {len(metrics['alerts'])}")
        EOF

    - name: Run KQL queries for detailed analysis
      run: |
        # Create KQL query file for this specific monitoring scenario
        cat > kql-query-${{ matrix.environment }}-${{ matrix.resource_type }}.kql << EOF
        // Infrastructure monitoring query for ${{ matrix.resource_type }} in ${{ matrix.environment }}
        let timeRange = ago(15m);
        let environment = "${{ matrix.environment }}";
        let resourceType = "${{ matrix.resource_type }}";
        
        // Query based on resource type and metric category
        union
        (
            // AKS monitoring
            KubePodInventory
            | where TimeGenerated > timeRange
            | where Namespace == "ai-multiagent-lab"
            | summarize 
                PodCount = dcount(Name),
                RunningPods = countif(PodStatus == "Running"),
                FailedPods = countif(PodStatus == "Failed")
                by bin(TimeGenerated, 5m)
        ),
        (
            // Cosmos DB monitoring
            AzureDiagnostics
            | where TimeGenerated > timeRange
            | where ResourceProvider == "MICROSOFT.DOCUMENTDB"
            | summarize 
                RequestCount = count(),
                AvgDuration = avg(duration_s),
                ErrorCount = countif(statusCode_s >= "400")
                by bin(TimeGenerated, 5m)
        ),
        (
            // Functions monitoring
            FunctionAppLogs
            | where TimeGenerated > timeRange
            | summarize 
                ExecutionCount = count(),
                SuccessCount = countif(Level != "Error"),
                ErrorCount = countif(Level == "Error")
                by bin(TimeGenerated, 5m)
        )
        | order by TimeGenerated desc
        EOF
        
        echo "KQL query created for ${{ matrix.resource_type }} monitoring"

    - name: Generate monitoring report
      run: |
        python << EOF
        import json
        import matplotlib.pyplot as plt
        import pandas as pd
        from datetime import datetime
        
        # Load metrics
        try:
            with open("metrics-${{ matrix.environment }}-${{ matrix.resource_type }}-${{ matrix.metric_category }}.json", "r") as f:
                metrics = json.load(f)
            
            # Generate simple visualization
            fig, ax = plt.subplots(figsize=(10, 6))
            
            # Create sample time series data
            times = pd.date_range(start=datetime.now().replace(minute=0, second=0, microsecond=0), 
                                periods=24, freq='H')
            values = [95 + i*0.1 + (i%3)*0.5 for i in range(24)]  # Mock data
            
            ax.plot(times, values, marker='o')
            ax.set_title(f"${{ matrix.resource_type }} ${{ matrix.metric_category }} - ${{ matrix.environment }}")
            ax.set_xlabel("Time")
            ax.set_ylabel("Metric Value")
            ax.grid(True)
            
            plt.xticks(rotation=45)
            plt.tight_layout()
            plt.savefig(f"monitoring-chart-${{ matrix.environment }}-${{ matrix.resource_type }}-${{ matrix.metric_category }}.png")
            plt.close()
            
            print(f"Monitoring chart generated for ${{ matrix.resource_type }}")
            
        except Exception as e:
            print(f"Error generating chart: {e}")
        EOF

    - name: Upload monitoring results
      uses: actions/upload-artifact@v3
      with:
        name: infrastructure-monitoring-${{ matrix.environment }}-${{ matrix.resource_type }}-${{ matrix.metric_category }}
        path: |
          metrics-*.json
          kql-query-*.kql
          monitoring-chart-*.png

  # Job 2: Application Monitoring Matrix
  application-monitoring:
    name: Application Monitoring
    runs-on: ubuntu-latest
    if: github.event.inputs.monitoring_scope == 'all' || github.event.inputs.monitoring_scope == 'applications'
    strategy:
      matrix:
        environment: [development, staging, production]
        agent: [coordinator, analysis-agent, generation-agent, validation-agent]
        metric_type: [health, performance, errors, business]
      fail-fast: false

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Login to Azure
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Monitor agent health
      run: |
        python << EOF
        import json
        import requests
        from datetime import datetime
        import time
        
        agent = "${{ matrix.agent }}"
        environment = "${{ matrix.environment }}"
        metric_type = "${{ matrix.metric_type }}"
        
        print(f"Monitoring {agent} {metric_type} in {environment}")
        
        # Mock agent monitoring
        health_data = {
            "timestamp": datetime.now().isoformat(),
            "agent": agent,
            "environment": environment,
            "metric_type": metric_type,
            "status": "healthy",
            "metrics": {},
            "alerts": []
        }
        
        if metric_type == "health":
            health_data["metrics"] = {
                "uptime": "99.9%",
                "last_heartbeat": datetime.now().isoformat(),
                "memory_usage": "45%",
                "cpu_usage": "23%"
            }
        elif metric_type == "performance":
            health_data["metrics"] = {
                "avg_response_time": "120ms",
                "requests_per_second": 45,
                "throughput": "1.2MB/s",
                "latency_p95": "250ms"
            }
        elif metric_type == "errors":
            health_data["metrics"] = {
                "error_rate": "0.1%",
                "total_errors": 3,
                "error_types": ["timeout", "validation"],
                "last_error": "2 hours ago"
            }
        elif metric_type == "business":
            health_data["metrics"] = {
                "tasks_completed": 1250,
                "success_rate": "99.2%",
                "avg_processing_time": "2.3s",
                "queue_length": 5
            }
        
        # Generate alerts based on thresholds
        if metric_type == "performance" and health_data["metrics"]["avg_response_time"] > "200ms":
            health_data["alerts"].append({
                "level": "warning",
                "message": f"High response time for {agent}"
            })
        
        with open(f"agent-metrics-{environment}-{agent}-{metric_type}.json", "w") as f:
            json.dump(health_data, f, indent=2)
        
        print(f"Agent monitoring data saved for {agent}")
        EOF

    - name: Execute KQL queries for agent monitoring
      run: |
        # Use the pre-created KQL queries from monitoring/queries/
        cp monitoring/queries/agent-monitoring.kql agent-monitoring-${{ matrix.agent }}.kql
        
        # Customize query for specific agent
        sed -i 's/let TargetAgent = "coordinator-agent-123"/let TargetAgent = "${{ matrix.agent }}"/g' agent-monitoring-${{ matrix.agent }}.kql
        
        echo "Customized KQL query for ${{ matrix.agent }} monitoring"

    - name: Upload application monitoring results
      uses: actions/upload-artifact@v3
      with:
        name: application-monitoring-${{ matrix.environment }}-${{ matrix.agent }}-${{ matrix.metric_type }}
        path: |
          agent-metrics-*.json
          agent-monitoring-*.kql

  # Job 3: Security Monitoring Matrix
  security-monitoring:
    name: Security Monitoring
    runs-on: ubuntu-latest
    if: github.event.inputs.monitoring_scope == 'all' || github.event.inputs.monitoring_scope == 'security'
    strategy:
      matrix:
        environment: [development, staging, production]
        security_domain: [authentication, authorization, data-protection, network-security]
        scan_type: [real-time, scheduled, on-demand]
      fail-fast: false

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Login to Azure
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Run security monitoring
      run: |
        python << EOF
        import json
        from datetime import datetime
        
        environment = "${{ matrix.environment }}"
        security_domain = "${{ matrix.security_domain }}"
        scan_type = "${{ matrix.scan_type }}"
        
        print(f"Security monitoring: {security_domain} ({scan_type}) in {environment}")
        
        # Mock security monitoring data
        security_data = {
            "timestamp": datetime.now().isoformat(),
            "environment": environment,
            "security_domain": security_domain,
            "scan_type": scan_type,
            "security_score": 85,
            "findings": [],
            "recommendations": [],
            "compliance_status": "compliant"
        }
        
        # Generate mock findings based on domain
        if security_domain == "authentication":
            security_data["findings"] = [
                {"severity": "low", "type": "weak_password_policy", "count": 2},
                {"severity": "medium", "type": "inactive_users", "count": 5}
            ]
        elif security_domain == "authorization":
            security_data["findings"] = [
                {"severity": "high", "type": "excessive_permissions", "count": 1},
                {"severity": "low", "type": "unused_roles", "count": 3}
            ]
        elif security_domain == "data-protection":
            security_data["findings"] = [
                {"severity": "medium", "type": "unencrypted_data", "count": 0},
                {"severity": "low", "type": "backup_issues", "count": 1}
            ]
        elif security_domain == "network-security":
            security_data["findings"] = [
                {"severity": "medium", "type": "open_ports", "count": 2},
                {"severity": "low", "type": "ssl_issues", "count": 1}
            ]
        
        # Generate recommendations
        if security_data["findings"]:
            security_data["recommendations"] = [
                f"Review {security_domain} configuration",
                f"Implement additional {security_domain} controls",
                f"Schedule regular {security_domain} audits"
            ]
        
        with open(f"security-monitoring-{environment}-{security_domain}-{scan_type}.json", "w") as f:
            json.dump(security_data, f, indent=2)
        
        print(f"Security monitoring completed for {security_domain}")
        EOF

    - name: Execute security KQL queries
      run: |
        # Use security monitoring queries
        cp monitoring/queries/security-monitoring.kql security-monitoring-${{ matrix.security_domain }}.kql
        
        echo "Security KQL queries prepared for ${{ matrix.security_domain }}"

    - name: Upload security monitoring results
      uses: actions/upload-artifact@v3
      with:
        name: security-monitoring-${{ matrix.environment }}-${{ matrix.security_domain }}-${{ matrix.scan_type }}
        path: |
          security-monitoring-*.json
          security-monitoring-*.kql

  # Job 4: Performance Monitoring Matrix
  performance-monitoring:
    name: Performance Monitoring
    runs-on: ubuntu-latest
    if: github.event.inputs.monitoring_scope == 'all' || github.event.inputs.monitoring_scope == 'performance'
    strategy:
      matrix:
        environment: [development, staging, production]
        performance_metric: [response-time, throughput, resource-usage, scalability]
        time_window: [15m, 1h, 6h, 24h]
      fail-fast: false

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Login to Azure
      uses: azure/login@v1
      with:
        creds: ${{ secrets.AZURE_CREDENTIALS }}

    - name: Monitor performance metrics
      run: |
        python << EOF
        import json
        import matplotlib.pyplot as plt
        import numpy as np
        from datetime import datetime, timedelta
        
        environment = "${{ matrix.environment }}"
        performance_metric = "${{ matrix.performance_metric }}"
        time_window = "${{ matrix.time_window }}"
        
        print(f"Performance monitoring: {performance_metric} over {time_window} in {environment}")
        
        # Generate mock performance data
        performance_data = {
            "timestamp": datetime.now().isoformat(),
            "environment": environment,
            "metric": performance_metric,
            "time_window": time_window,
            "data_points": [],
            "summary": {},
            "alerts": []
        }
        
        # Generate time series data
        if time_window == "15m":
            points = 15
        elif time_window == "1h":
            points = 60
        elif time_window == "6h":
            points = 72
        else:  # 24h
            points = 144
        
        # Mock data generation based on metric type
        if performance_metric == "response-time":
            base_value = 120
            values = [base_value + np.random.normal(0, 20) for _ in range(points)]
        elif performance_metric == "throughput":
            base_value = 1000
            values = [base_value + np.random.normal(0, 100) for _ in range(points)]
        elif performance_metric == "resource-usage":
            base_value = 45
            values = [max(0, min(100, base_value + np.random.normal(0, 10))) for _ in range(points)]
        else:  # scalability
            base_value = 50
            values = [base_value + np.random.normal(0, 5) for _ in range(points)]
        
        # Create data points
        start_time = datetime.now() - timedelta(hours=24 if time_window == "24h" else 6 if time_window == "6h" else 1)
        for i, value in enumerate(values):
            timestamp = start_time + timedelta(minutes=i * (1440//points if time_window == "24h" else 360//points if time_window == "6h" else 60//points))
            performance_data["data_points"].append({
                "timestamp": timestamp.isoformat(),
                "value": round(value, 2)
            })
        
        # Calculate summary statistics
        performance_data["summary"] = {
            "min": round(min(values), 2),
            "max": round(max(values), 2),
            "avg": round(np.mean(values), 2),
            "p95": round(np.percentile(values, 95), 2),
            "p99": round(np.percentile(values, 99), 2)
        }
        
        # Generate alerts based on thresholds
        if performance_metric == "response-time" and performance_data["summary"]["avg"] > 200:
            performance_data["alerts"].append({
                "level": "warning",
                "message": f"Average response time high: {performance_data['summary']['avg']}ms"
            })
        
        # Create visualization
        plt.figure(figsize=(12, 6))
        timestamps = [datetime.fromisoformat(dp["timestamp"]) for dp in performance_data["data_points"]]
        values = [dp["value"] for dp in performance_data["data_points"]]
        
        plt.plot(timestamps, values, marker='o', markersize=2)
        plt.title(f"{performance_metric} over {time_window} - {environment}")
        plt.xlabel("Time")
        plt.ylabel(f"{performance_metric}")
        plt.grid(True, alpha=0.3)
        plt.xticks(rotation=45)
        plt.tight_layout()
        plt.savefig(f"performance-chart-{environment}-{performance_metric}-{time_window}.png", dpi=150)
        plt.close()
        
        # Save data
        with open(f"performance-data-{environment}-{performance_metric}-{time_window}.json", "w") as f:
            json.dump(performance_data, f, indent=2)
        
        print(f"Performance monitoring completed for {performance_metric}")
        print(f"Summary: avg={performance_data['summary']['avg']}, p95={performance_data['summary']['p95']}")
        EOF

    - name: Execute performance KQL queries
      run: |
        # Use performance analysis queries
        cp monitoring/queries/performance-analysis.kql performance-analysis-${{ matrix.performance_metric }}.kql
        
        echo "Performance KQL queries prepared for ${{ matrix.performance_metric }}"

    - name: Upload performance monitoring results
      uses: actions/upload-artifact@v3
      with:
        name: performance-monitoring-${{ matrix.environment }}-${{ matrix.performance_metric }}-${{ matrix.time_window }}
        path: |
          performance-data-*.json
          performance-chart-*.png
          performance-analysis-*.kql

  # Job 5: Alert Aggregation and Notification
  alert-aggregation:
    name: Alert Aggregation and Notification
    runs-on: ubuntu-latest
    needs: [infrastructure-monitoring, application-monitoring, security-monitoring, performance-monitoring]
    if: always()

    steps:
    - name: Checkout code
      uses: actions/checkout@v4

    - name: Download all monitoring artifacts
      uses: actions/download-artifact@v3

    - name: Aggregate alerts
      run: |
        python << EOF
        import json
        import os
        from datetime import datetime
        from collections import defaultdict
        
        print("Aggregating alerts from all monitoring jobs...")
        
        alerts_summary = {
            "timestamp": datetime.now().isoformat(),
            "total_alerts": 0,
            "alerts_by_level": defaultdict(int),
            "alerts_by_environment": defaultdict(int),
            "alerts_by_category": defaultdict(int),
            "critical_alerts": [],
            "recommendations": []
        }
        
        # Process all JSON files to extract alerts
        for root, dirs, files in os.walk("."):
            for file in files:
                if file.endswith(".json") and ("metrics" in file or "monitoring" in file):
                    try:
                        with open(os.path.join(root, file), "r") as f:
                            data = json.load(f)
                        
                        if "alerts" in data and data["alerts"]:
                            for alert in data["alerts"]:
                                alerts_summary["total_alerts"] += 1
                                alerts_summary["alerts_by_level"][alert.get("level", "unknown")] += 1
                                
                                if "environment" in data:
                                    alerts_summary["alerts_by_environment"][data["environment"]] += 1
                                
                                # Categorize alerts
                                if "agent" in data:
                                    alerts_summary["alerts_by_category"]["application"] += 1
                                elif "resource_type" in data:
                                    alerts_summary["alerts_by_category"]["infrastructure"] += 1
                                elif "security_domain" in data:
                                    alerts_summary["alerts_by_category"]["security"] += 1
                                elif "performance_metric" in data:
                                    alerts_summary["alerts_by_category"]["performance"] += 1
                                
                                # Collect critical alerts
                                if alert.get("level") == "critical":
                                    alerts_summary["critical_alerts"].append({
                                        "source": file,
                                        "message": alert.get("message", ""),
                                        "timestamp": data.get("timestamp", "")
                                    })
                    
                    except Exception as e:
                        print(f"Error processing {file}: {e}")
        
        # Generate recommendations
        if alerts_summary["total_alerts"] > 10:
            alerts_summary["recommendations"].append("High number of alerts detected - review monitoring thresholds")
        
        if alerts_summary["alerts_by_level"]["critical"] > 0:
            alerts_summary["recommendations"].append("Critical alerts require immediate attention")
        
        if alerts_summary["alerts_by_category"]["security"] > 0:
            alerts_summary["recommendations"].append("Security alerts detected - review security posture")
        
        # Save aggregated alerts
        with open("alerts-summary.json", "w") as f:
            json.dump(alerts_summary, f, indent=2)
        
        print(f"Alert aggregation completed:")
        print(f"- Total alerts: {alerts_summary['total_alerts']}")
        print(f"- Critical alerts: {alerts_summary['alerts_by_level']['critical']}")
        print(f"- Recommendations: {len(alerts_summary['recommendations'])}")
        EOF

    - name: Generate monitoring dashboard
      run: |
        python << EOF
        import json
        import matplotlib.pyplot as plt
        import numpy as np
        
        # Load alerts summary
        with open("alerts-summary.json", "r") as f:
            alerts_summary = json.load(f)
        
        # Create dashboard with multiple subplots
        fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))
        
        # Alerts by level
        levels = list(alerts_summary["alerts_by_level"].keys())
        counts = list(alerts_summary["alerts_by_level"].values())
        if levels:
            ax1.pie(counts, labels=levels, autopct='%1.1f%%')
            ax1.set_title("Alerts by Level")
        else:
            ax1.text(0.5, 0.5, "No alerts", ha='center', va='center', transform=ax1.transAxes)
            ax1.set_title("Alerts by Level")
        
        # Alerts by environment
        environments = list(alerts_summary["alerts_by_environment"].keys())
        env_counts = list(alerts_summary["alerts_by_environment"].values())
        if environments:
            ax2.bar(environments, env_counts)
            ax2.set_title("Alerts by Environment")
            ax2.set_ylabel("Count")
        else:
            ax2.text(0.5, 0.5, "No alerts", ha='center', va='center', transform=ax2.transAxes)
            ax2.set_title("Alerts by Environment")
        
        # Alerts by category
        categories = list(alerts_summary["alerts_by_category"].keys())
        cat_counts = list(alerts_summary["alerts_by_category"].values())
        if categories:
            ax3.bar(categories, cat_counts)
            ax3.set_title("Alerts by Category")
            ax3.set_ylabel("Count")
            ax3.tick_params(axis='x', rotation=45)
        else:
            ax3.text(0.5, 0.5, "No alerts", ha='center', va='center', transform=ax3.transAxes)
            ax3.set_title("Alerts by Category")
        
        # Summary statistics
        ax4.axis('off')
        summary_text = f"""
        Monitoring Summary
        
        Total Alerts: {alerts_summary["total_alerts"]}
        Critical: {alerts_summary["alerts_by_level"].get("critical", 0)}
        Warning: {alerts_summary["alerts_by_level"].get("warning", 0)}
        Info: {alerts_summary["alerts_by_level"].get("info", 0)}
        
        Recommendations: {len(alerts_summary["recommendations"])}
        """
        ax4.text(0.1, 0.9, summary_text, transform=ax4.transAxes, fontsize=12, 
                verticalalignment='top', fontfamily='monospace')
        
        plt.tight_layout()
        plt.savefig("monitoring-dashboard.png", dpi=150, bbox_inches='tight')
        plt.close()
        
        print("Monitoring dashboard generated")
        EOF

    - name: Create monitoring report
      run: |
        cat > monitoring-report.md << EOF
        # Monitoring and Alerting Report
        
        **Generated:** $(date)
        **Scope:** ${{ github.event.inputs.monitoring_scope || 'all' }}
        **Environment:** ${{ github.event.inputs.environment || 'all' }}
        
        ## Executive Summary
        
        This report provides a comprehensive overview of the AI Multiagent Lab monitoring status across all environments and components.
        
        ## Alert Summary
        
        $(python3 -c "
        import json
        with open('alerts-summary.json', 'r') as f:
            data = json.load(f)
        print(f'- **Total Alerts:** {data[\"total_alerts\"]}')
        print(f'- **Critical:** {data[\"alerts_by_level\"].get(\"critical\", 0)}')
        print(f'- **Warning:** {data[\"alerts_by_level\"].get(\"warning\", 0)}')
        print(f'- **Info:** {data[\"alerts_by_level\"].get(\"info\", 0)}')
        ")
        
        ## Monitoring Coverage
        
        - ✅ Infrastructure Monitoring: ${{ needs.infrastructure-monitoring.result }}
        - ✅ Application Monitoring: ${{ needs.application-monitoring.result }}
        - ✅ Security Monitoring: ${{ needs.security-monitoring.result }}
        - ✅ Performance Monitoring: ${{ needs.performance-monitoring.result }}
        
        ## Recommendations
        
        $(python3 -c "
        import json
        with open('alerts-summary.json', 'r') as f:
            data = json.load(f)
        for i, rec in enumerate(data['recommendations'], 1):
            print(f'{i}. {rec}')
        ")
        
        ## Next Steps
        
        1. Review critical alerts and take immediate action
        2. Analyze trends in warning alerts
        3. Update monitoring thresholds if needed
        4. Schedule follow-up monitoring review
        
        ---
        *This report was automatically generated by the AI Multiagent Lab monitoring pipeline.*
        EOF

    - name: Upload monitoring report
      uses: actions/upload-artifact@v3
      with:
        name: monitoring-report-$(date +%Y%m%d-%H%M%S)
        path: |
          alerts-summary.json
          monitoring-dashboard.png
          monitoring-report.md

    - name: Send notifications (if critical alerts)
      run: |
        python << EOF
        import json
        
        with open("alerts-summary.json", "r") as f:
            alerts_summary = json.load(f)
        
        critical_count = alerts_summary["alerts_by_level"].get("critical", 0)
        
        if critical_count > 0:
            print(f"🚨 CRITICAL ALERTS DETECTED: {critical_count}")
            print("Critical alerts require immediate attention!")
            
            # In a real scenario, this would send notifications via:
            # - Email
            # - Slack/Teams
            # - SMS
            # - PagerDuty
            # - Azure Monitor Action Groups
            
            for alert in alerts_summary["critical_alerts"]:
                print(f"- {alert['message']} (Source: {alert['source']})")
        else:
            print("✅ No critical alerts detected")
        
        total_alerts = alerts_summary["total_alerts"]
        if total_alerts > 0:
            print(f"📊 Total alerts: {total_alerts}")
        else:
            print("🎉 All systems healthy - no alerts!")
        EOF

